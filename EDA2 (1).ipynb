{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d55bc790-94d8-461e-a30f-277bc78f2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# DATA PREPROCESSING & FEATURE ENGINEERING â€” Adult Dataset\n",
    "# =========================================================\n",
    "# This single cell is organized into clear \"steps\" for Jupyter.\n",
    "# If you prefer, split each STEP into separate notebook cells.\n",
    "# =========================================================\n",
    "\n",
    "# ---------- STEP 0. Imports & Config ----------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import mutual_info_classif  # <-- FIXED: correct import\n",
    "\n",
    "# Robust OHE across sklearn versions (sparse_output vs sparse)\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        # Older sklearn versions use `sparse` instead of `sparse_output`\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# Use YOUR Windows path (keep the r prefix!)\n",
    "DATA_PATH = r\"C:\\Users\\user\\Data-Science-main\\adult_with_headers.csv\"\n",
    "assert os.path.exists(DATA_PATH), f\"File not found at {DATA_PATH}. Update DATA_PATH.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3af17cae-62a4-4df5-9bff-b24d6a6d315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected TARGET_COL: 'income'\n",
      "\n",
      "=== SHAPE ===\n",
      "(32561, 15)\n",
      "\n",
      "=== DTYPE INFO ===\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education_num      int64\n",
      "marital_status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital_gain       int64\n",
      "capital_loss       int64\n",
      "hours_per_week     int64\n",
      "native_country    object\n",
      "income            object\n",
      "dtype: object\n",
      "\n",
      "=== MISSING VALUES (count) ===\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education_num     0\n",
      "marital_status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital_gain      0\n",
      "capital_loss      0\n",
      "hours_per_week    0\n",
      "native_country    0\n",
      "income            0\n",
      "dtype: int64\n",
      "\n",
      "=== TARGET DISTRIBUTION ===\n",
      "income\n",
      "<=50K    24720\n",
      ">50K      7841\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== NUMERIC SUMMARY (describe) ===\n",
      "                  count           mean            std      min       25%  \\\n",
      "age             32561.0      38.581647      13.640433     17.0      28.0   \n",
      "fnlwgt          32561.0  189778.366512  105549.977697  12285.0  117827.0   \n",
      "education_num   32561.0      10.080679       2.572720      1.0       9.0   \n",
      "capital_gain    32561.0    1077.648844    7385.292085      0.0       0.0   \n",
      "capital_loss    32561.0      87.303830     402.960219      0.0       0.0   \n",
      "hours_per_week  32561.0      40.437456      12.347429      1.0      40.0   \n",
      "\n",
      "                     50%       75%        max  \n",
      "age                 37.0      48.0       90.0  \n",
      "fnlwgt          178356.0  237051.0  1484705.0  \n",
      "education_num       10.0      12.0       16.0  \n",
      "capital_gain         0.0       0.0    99999.0  \n",
      "capital_loss         0.0       0.0     4356.0  \n",
      "hours_per_week      40.0      45.0       99.0  \n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 1. Load & Basic EDA ----------\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Clean headers\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Replace \"?\" (Adult dataset placeholder) with NaN; strip whitespace\n",
    "df.replace(\"?\", np.nan, inplace=True)\n",
    "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[c] = df[c].astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "\n",
    "# Try to identify the target column robustly (commonly 'income')\n",
    "possible_targets = [c for c in df.columns if \"income\" in c.lower() or c.lower() in [\"target\", \"class\"]]\n",
    "TARGET_COL = possible_targets[0] if possible_targets else df.columns[-1]\n",
    "print(f\"Detected TARGET_COL: {TARGET_COL!r}\")\n",
    "\n",
    "print(\"\\n=== SHAPE ===\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\n=== DTYPE INFO ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== MISSING VALUES (count) ===\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\n=== TARGET DISTRIBUTION ===\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False))\n",
    "\n",
    "# Coerce likely numeric columns just in case they were parsed as object\n",
    "likely_numeric = [\"age\",\"fnlwgt\",\"education-num\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"]\n",
    "for col in likely_numeric:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "print(\"\\n=== NUMERIC SUMMARY (describe) ===\")\n",
    "print(df.select_dtypes(include=[np.number]).describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15bc7711-be2d-4154-a224-a601e8294a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NUMERIC COLS ===\n",
      "['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
      "\n",
      "=== CATEGORICAL COLS ===\n",
      "['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "\n",
      "Imputation complete (preview):\n",
      "    age         workclass    fnlwgt  education  education_num  \\\n",
      "0  39.0         State-gov   77516.0  Bachelors           13.0   \n",
      "1  50.0  Self-emp-not-inc   83311.0  Bachelors           13.0   \n",
      "2  38.0           Private  215646.0    HS-grad            9.0   \n",
      "3  53.0           Private  234721.0       11th            7.0   \n",
      "4  28.0           Private  338409.0  Bachelors           13.0   \n",
      "\n",
      "       marital_status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital_gain  capital_loss  hours_per_week native_country income  \n",
      "0        2174.0           0.0            40.0  United-States  <=50K  \n",
      "1           0.0           0.0            13.0  United-States  <=50K  \n",
      "2           0.0           0.0            40.0  United-States  <=50K  \n",
      "3           0.0           0.0            40.0  United-States  <=50K  \n",
      "4           0.0           0.0            40.0           Cuba  <=50K  \n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 2. Handle Missing Values ----------\n",
    "# Define imputers (best practices)\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Separate features by dtype\n",
    "all_features = [c for c in df.columns if c != TARGET_COL]\n",
    "num_cols = [c for c in all_features if pd.api.types.is_numeric_dtype(df[c])]\n",
    "cat_cols = [c for c in all_features if c not in num_cols]\n",
    "\n",
    "print(\"\\n=== NUMERIC COLS ===\")\n",
    "print(num_cols)\n",
    "print(\"\\n=== CATEGORICAL COLS ===\")\n",
    "print(cat_cols)\n",
    "\n",
    "# Apply imputation for an imputed working copy (Pipelines will re-impute later)\n",
    "df_imputed = df.copy()\n",
    "df_imputed[num_cols] = num_imputer.fit_transform(df_imputed[num_cols])\n",
    "df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])\n",
    "\n",
    "print(\"\\nImputation complete (preview):\")\n",
    "print(df_imputed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f20b2ed1-e46d-48eb-8eb0-5a69f9d49f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCALING PREVIEW (first 3 rows of each) ===\n",
      "   age__std  fnlwgt__std  education_num__std  capital_gain__std  \\\n",
      "0  0.030671    -1.063611            1.134739           0.148453   \n",
      "1  0.837109    -1.008707            1.134739          -0.145920   \n",
      "2 -0.042642     0.245079           -0.420060          -0.145920   \n",
      "\n",
      "   capital_loss__std  hours_per_week__std   age__mm  fnlwgt__mm  \\\n",
      "0           -0.21666            -0.035429  0.301370    0.044302   \n",
      "1           -0.21666            -2.222153  0.452055    0.048238   \n",
      "2           -0.21666            -0.035429  0.287671    0.138113   \n",
      "\n",
      "   education_num__mm  capital_gain__mm  capital_loss__mm  hours_per_week__mm  \n",
      "0           0.800000           0.02174               0.0            0.397959  \n",
      "1           0.800000           0.00000               0.0            0.122449  \n",
      "2           0.533333           0.00000               0.0            0.397959  \n",
      "\n",
      "When to use which scaling:\n",
      "\n",
      "- StandardScaler: centers to mean 0 & unit variance. Good for models using dot products/distances or Gaussian assumptions\n",
      "  (Logistic/Linear Regression, SVM, PCA, k-means, KNN).\n",
      "- MinMaxScaler: rescales to [0,1]. Good when bounds matter, to preserve zero/positivity, or for neural nets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 3. Scaling: Standard vs MinMax ----------\n",
    "scaler_std = StandardScaler()\n",
    "scaler_mm  = MinMaxScaler()\n",
    "\n",
    "num_std = pd.DataFrame(\n",
    "    scaler_std.fit_transform(df_imputed[num_cols]),\n",
    "    columns=[f\"{c}__std\" for c in num_cols]\n",
    ")\n",
    "num_mm  = pd.DataFrame(\n",
    "    scaler_mm.fit_transform(df_imputed[num_cols]),\n",
    "    columns=[f\"{c}__mm\" for c in num_cols]\n",
    ")\n",
    "\n",
    "print(\"\\n=== SCALING PREVIEW (first 3 rows of each) ===\")\n",
    "print(pd.concat([num_std.head(3), num_mm.head(3)], axis=1))\n",
    "\n",
    "print(\"\\nWhen to use which scaling:\")\n",
    "print(\"\"\"\n",
    "- StandardScaler: centers to mean 0 & unit variance. Good for models using dot products/distances or Gaussian assumptions\n",
    "  (Logistic/Linear Regression, SVM, PCA, k-means, KNN).\n",
    "- MinMaxScaler: rescales to [0,1]. Good when bounds matter, to preserve zero/positivity, or for neural nets.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01790fdf-36ec-4176-8248-51f72661d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Low-cardinality categorical columns (OHE): ['sex']\n",
      "High-cardinality categorical columns (Ordinal): ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'native_country']\n",
      "\n",
      "Pros/Cons:\n",
      "\n",
      "One-Hot Encoding (OHE)\n",
      "  + No artificial ordering; safe for linear & tree models.\n",
      "  + Great for low-cardinality features.\n",
      "  - Can blow up dimensionality with many categories.\n",
      "  - More memory (often sparse).\n",
      "\n",
      "Label/Ordinal Encoding\n",
      "  + Compact (one column).\n",
      "  + Useful for high-cardinality features to avoid OHE explosion.\n",
      "  - Imposes arbitrary order; can mislead linear/distance-based models.\n",
      "  - Trees tolerate this better, but still be cautious.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 4. Encoding: OHE (<5 cats) & Label/Ordinal (>=5 cats) ----------\n",
    "# We'll treat label encoding as per-feature OrdinalEncoder (LabelEncoder is 1D and for targets).\n",
    "low_card_cats  = [c for c in cat_cols if df_imputed[c].nunique(dropna=True) < 5]\n",
    "high_card_cats = [c for c in cat_cols if c not in low_card_cats]\n",
    "\n",
    "print(\"\\nLow-cardinality categorical columns (OHE):\", low_card_cats)\n",
    "print(\"High-cardinality categorical columns (Ordinal):\", high_card_cats)\n",
    "\n",
    "ohe = make_ohe()\n",
    "ord_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "# A base transformer (for demonstration/preview)\n",
    "pre_base = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_imputer, num_cols),\n",
    "        (\"ohe\", Pipeline([(\"impute\", cat_imputer), (\"enc\", ohe)]), low_card_cats),\n",
    "        (\"ord\", Pipeline([(\"impute\", cat_imputer), (\"enc\", ord_enc)]), high_card_cats),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "print(\"\\nPros/Cons:\")\n",
    "print(\"\"\"\n",
    "One-Hot Encoding (OHE)\n",
    "  + No artificial ordering; safe for linear & tree models.\n",
    "  + Great for low-cardinality features.\n",
    "  - Can blow up dimensionality with many categories.\n",
    "  - More memory (often sparse).\n",
    "\n",
    "Label/Ordinal Encoding\n",
    "  + Compact (one column).\n",
    "  + Useful for high-cardinality features to avoid OHE explosion.\n",
    "  - Imposes arbitrary order; can mislead linear/distance-based models.\n",
    "  - Trees tolerate this better, but still be cautious.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7ee2ea6-d7d0-4615-a0ee-abc38f152ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE ENGINEERING â€” RATIONALE ===\n",
      "1) capital_net = capital_gain - capital_loss\n",
      "   Why: condenses two skewed investment-related features into a single net wealth signal.\n",
      "\n",
      "2) is_married (1 if marital-status contains 'Married' else 0)\n",
      "   Why: captures household/relationship factors often related to income.\n",
      "\n",
      "3) age_bucket (young/adult/mid_aged/senior)\n",
      "   Why: models non-linear income trends across career stages without forcing linearity.\n",
      "\n",
      "4) long_hours (1 if hours-per-week â‰¥ 50)\n",
      "   Why: indicates higher work intensity, often associated with higher earnings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 5. Feature Engineering (WITH EXPLANATIONS) ----------\n",
    "df_fe = df_imputed.copy()\n",
    "\n",
    "# (F1) capital_net = capital-gain - capital-loss\n",
    "# Rationale: condenses two skewed monetary signals into one net indicator of investment income â†’ predictive of >50K.\n",
    "if \"capital-gain\" in df_fe.columns and \"capital-loss\" in df_fe.columns:\n",
    "    df_fe[\"capital_net\"] = df_fe[\"capital-gain\"].fillna(0) - df_fe[\"capital-loss\"].fillna(0)\n",
    "else:\n",
    "    df_fe[\"capital_net\"] = 0.0\n",
    "\n",
    "# (F2) is_married (binary from 'marital-status')\n",
    "# Rationale: marital status correlates with household structure & income; married often associates with higher income in census data.\n",
    "if \"marital-status\" in df.columns:\n",
    "    df_fe[\"is_married\"] = df[\"marital-status\"].astype(str).str.contains(\"Married\", na=False).astype(int)\n",
    "else:\n",
    "    df_fe[\"is_married\"] = 0\n",
    "\n",
    "# (F3) age_bucket (categorical bins)\n",
    "# Rationale: captures non-linear earning patterns across life stages (young/adult/mid_aged/senior).\n",
    "if \"age\" in df_fe.columns:\n",
    "    df_fe[\"age_bucket\"] = pd.cut(\n",
    "        df_fe[\"age\"],\n",
    "        bins=[0, 25, 45, 65, np.inf],\n",
    "        labels=[\"young\", \"adult\", \"mid_aged\", \"senior\"],\n",
    "        right=False\n",
    "    )\n",
    "\n",
    "# (F4) long_hours (binary: â‰¥50 hrs/week)\n",
    "# Rationale: proxies work intensity/overtime which can map to higher salary bands.\n",
    "if \"hours-per-week\" in df_fe.columns:\n",
    "    df_fe[\"long_hours\"] = (df_fe[\"hours-per-week\"] >= 50).astype(int)\n",
    "\n",
    "print(\"\\n=== FEATURE ENGINEERING â€” RATIONALE ===\")\n",
    "print(\"\"\"\\\n",
    "1) capital_net = capital_gain - capital_loss\n",
    "   Why: condenses two skewed investment-related features into a single net wealth signal.\n",
    "\n",
    "2) is_married (1 if marital-status contains 'Married' else 0)\n",
    "   Why: captures household/relationship factors often related to income.\n",
    "\n",
    "3) age_bucket (young/adult/mid_aged/senior)\n",
    "   Why: models non-linear income trends across career stages without forcing linearity.\n",
    "\n",
    "4) long_hours (1 if hours-per-week â‰¥ 50)\n",
    "   Why: indicates higher work intensity, often associated with higher earnings.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6592eb69-1caf-43a2-8f99-5455fb3efaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated low/high-card categorical splits after FE:\n",
      "Low-card (OHE): ['sex', 'age_bucket']\n",
      "High-card (Ordinal): ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'native_country']\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 6. Transform a skewed feature (log) ----------\n",
    "# capital-gain is strongly right-skewed in Adult; log1p stabilizes variance & reduces outlier leverage.\n",
    "if \"capital-gain\" in df_fe.columns:\n",
    "    df_fe[\"capital_gain_log\"] = np.log1p(df_fe[\"capital-gain\"].clip(lower=0))\n",
    "    if \"capital_gain_log\" not in num_cols:\n",
    "        num_cols.append(\"capital_gain_log\")\n",
    "    print(\"\\nApplied log1p to 'capital-gain' â†’ 'capital_gain_log' due to strong right skew.\")\n",
    "\n",
    "# Prepare X, y\n",
    "y = df_fe[TARGET_COL].copy()\n",
    "X = df_fe.drop(columns=[TARGET_COL])\n",
    "\n",
    "# Recompute categorical candidates (after FE)\n",
    "new_cat_cols = [c for c in X.columns if not pd.api.types.is_numeric_dtype(X[c])]\n",
    "low_card_cats  = [c for c in new_cat_cols if X[c].astype(\"object\").nunique(dropna=True) < 5]\n",
    "high_card_cats = [c for c in new_cat_cols if c not in low_card_cats]\n",
    "\n",
    "print(\"\\nUpdated low/high-card categorical splits after FE:\")\n",
    "print(\"Low-card (OHE):\", low_card_cats)\n",
    "print(\"High-card (Ordinal):\", high_card_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33519912-4af5-468f-b373-08946f21e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IsolationForest flagged 3879 potential outliers out of 32561 rows.\n",
      "Outliers removed. Clean shapes: (28682, 17) (28682,)\n",
      "Note: Outliers can distort parameter estimates and distance-based models; removing them often stabilizes training.\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 7. Outlier Detection: IsolationForest ----------\n",
    "# Use numeric-only snapshot (already imputed) for outlier detection\n",
    "X_num_only = X.select_dtypes(include=[np.number]).copy()\n",
    "X_num_only = pd.DataFrame(SimpleImputer(strategy=\"median\").fit_transform(X_num_only), columns=X_num_only.columns)\n",
    "\n",
    "iso = IsolationForest(random_state=42, contamination=\"auto\")\n",
    "outlier_flag = iso.fit_predict(X_num_only)   # -1 outlier, 1 inlier\n",
    "inlier_mask = outlier_flag == 1\n",
    "\n",
    "print(f\"\\nIsolationForest flagged {np.sum(~inlier_mask)} potential outliers out of {len(inlier_mask)} rows.\")\n",
    "\n",
    "X_clean = X.loc[inlier_mask].reset_index(drop=True)\n",
    "y_clean = y.loc[inlier_mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Outliers removed. Clean shapes:\", X_clean.shape, y_clean.shape)\n",
    "print(\"Note: Outliers can distort parameter estimates and distance-based models; removing them often stabilizes training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ebba433-17b2-4331-8d43-41a78f277214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Note] ppscore not available. Install with:  pip install ppscore\n",
      "Proceeding with Mutual Information as a robust fallback.\n",
      "\n",
      "\n",
      "=== Top numeric correlations with target (absolute) ===\n",
      "education_num     0.296020\n",
      "capital_gain      0.250708\n",
      "age               0.232649\n",
      "hours_per_week    0.212993\n",
      "fnlwgt           -0.008689\n",
      "capital_loss      0.001005\n",
      "capital_net            NaN\n",
      "is_married             NaN\n",
      "Name: __target__, dtype: float64\n",
      "\n",
      "[Fallback] Top features by Mutual Information (proxy for PPS):\n",
      "                feature        MI\n",
      "16       marital_status  0.108651\n",
      "18         relationship  0.107793\n",
      "0                   age  0.059758\n",
      "17           occupation  0.053070\n",
      "2         education_num  0.049875\n",
      "15            education  0.048107\n",
      "3          capital_gain  0.047880\n",
      "13     age_bucket_young  0.044055\n",
      "5        hours_per_week  0.033388\n",
      "1                fnlwgt  0.028061\n",
      "8            sex_Female  0.024591\n",
      "9              sex_Male  0.023257\n",
      "14            workclass  0.013812\n",
      "11  age_bucket_mid_aged  0.013011\n",
      "20       native_country  0.011363\n",
      "\n",
      "How PPS vs Correlation differ?\n",
      "\n",
      "- Pearson correlation: linear-only, numeric-only, symmetric (X~Y).\n",
      "- PPS: model-based, directional (Xâ†’Y), handles categorical & non-linear relations.\n",
      "- Expect features with weak linear correlation to still rank high on PPS/MI if they are informative non-linearly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 8. PPS (Predictive Power Score) vs Correlation ----------\n",
    "# PPS can fail if ppscore package isn't installed; fall back to Mutual Information (MI).\n",
    "pps_available = False\n",
    "try:\n",
    "    import ppscore as pps\n",
    "    pps_available = True\n",
    "except Exception:\n",
    "    print(\"\\n[Note] ppscore not available. Install with:  pip install ppscore\")\n",
    "    print(\"Proceeding with Mutual Information as a robust fallback.\\n\")\n",
    "\n",
    "# Helper: map target to binary for correlation\n",
    "def to_binary_series(s):\n",
    "    ss = s.astype(str).str.strip()\n",
    "    # Adult: ' >50K' vs ' <=50K' (with or without spaces)\n",
    "    if ss.str.contains(\">50\", regex=False).any():\n",
    "        return (ss.str.contains(\">50\", regex=False)).astype(int)\n",
    "    # If exactly two classes, map to 0/1\n",
    "    vals = ss.unique().tolist()\n",
    "    if len(vals) == 2:\n",
    "        m = {vals[0]:0, vals[1]:1}\n",
    "        return ss.map(m).astype(int)\n",
    "    # Multi-class fallback\n",
    "    return pd.factorize(ss)[0]\n",
    "\n",
    "# 8a) Correlation (numeric only, Pearson)\n",
    "y_bin = to_binary_series(y_clean)\n",
    "num_only_clean = X_clean.select_dtypes(include=[np.number]).copy()\n",
    "corr_df = pd.concat([num_only_clean, y_bin.rename(\"__target__\")], axis=1)\n",
    "corr_series = corr_df.corr(numeric_only=True)[\"__target__\"].drop(\"__target__\", errors=\"ignore\")\n",
    "corr_top = corr_series.sort_values(key=lambda s: s.abs(), ascending=False).head(10)\n",
    "\n",
    "print(\"\\n=== Top numeric correlations with target (absolute) ===\")\n",
    "print(corr_top)\n",
    "\n",
    "# 8b) PPS if available; else MI on an encoded view of X_clean\n",
    "if pps_available:\n",
    "    # PPS expects a DataFrame with both features and the target column present\n",
    "    df_pps = X_clean.copy()\n",
    "    df_pps[y_clean.name] = y_clean\n",
    "\n",
    "    pps_scores = []\n",
    "    for col in X_clean.columns:\n",
    "        try:\n",
    "            sc = pps.score(df_pps, col, y_clean.name)[\"ppscore\"]\n",
    "        except Exception:\n",
    "            sc = np.nan\n",
    "        pps_scores.append((col, sc))\n",
    "    pps_df = pd.DataFrame(pps_scores, columns=[\"feature\", \"pps\"]).sort_values(\"pps\", ascending=False)\n",
    "    print(\"\\n=== Top PPS features (feature â†’ target) ===\")\n",
    "    print(pps_df.head(15))\n",
    "else:\n",
    "    # Mutual Information fallback (continuous + categorical, non-linear capable)\n",
    "    ohe = make_ohe()\n",
    "    ord_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "    enc_pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", \"passthrough\", [c for c in X_clean.columns if pd.api.types.is_numeric_dtype(X_clean[c])]),\n",
    "            (\"ohe\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")), (\"enc\", ohe)]), low_card_cats),\n",
    "            (\"ord\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")), (\"enc\", ord_enc)]), high_card_cats),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    X_enc = enc_pre.fit_transform(X_clean)\n",
    "    if hasattr(X_enc, \"toarray\"):\n",
    "        X_enc = X_enc.toarray()\n",
    "    mi_vals = mutual_info_classif(X_enc, y_bin, discrete_features=False, random_state=42)\n",
    "    try:\n",
    "        feat_names = enc_pre.get_feature_names_out()\n",
    "    except Exception:\n",
    "        feat_names = [f\"f{i}\" for i in range(X_enc.shape[1])]\n",
    "    mi_df = pd.DataFrame({\"feature\": feat_names, \"MI\": mi_vals}).sort_values(\"MI\", ascending=False)\n",
    "    print(\"\\n[Fallback] Top features by Mutual Information (proxy for PPS):\")\n",
    "    print(mi_df.head(15))\n",
    "\n",
    "print(\"\\nHow PPS vs Correlation differ?\")\n",
    "print(\"\"\"\n",
    "- Pearson correlation: linear-only, numeric-only, symmetric (X~Y).\n",
    "- PPS: model-based, directional (Xâ†’Y), handles categorical & non-linear relations.\n",
    "- Expect features with weak linear correlation to still rank high on PPS/MI if they are informative non-linearly.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f68aaefd-05f9-46d9-a3d5-3d9e2216c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final preprocessed feature matrix shape: (28682, 21)\n",
      "\n",
      "All steps complete \n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 9. Final End-to-End Preprocessor (ready for modeling) ----------\n",
    "# Choose your scaler here (we'll use StandardScaler in the final pipeline)\n",
    "final_num_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "final_ohe_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"enc\", make_ohe())\n",
    "])\n",
    "final_ord_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "])\n",
    "\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", final_num_pipe, [c for c in X_clean.columns if pd.api.types.is_numeric_dtype(X_clean[c])]),\n",
    "        (\"ohe\", final_ohe_pipe, low_card_cats),\n",
    "        (\"ord\", final_ord_pipe, high_card_cats),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=True,\n",
    ")\n",
    "\n",
    "X_ready = final_preprocessor.fit_transform(X_clean)\n",
    "print(\"\\nFinal preprocessed feature matrix shape:\", X_ready.shape)\n",
    "print(\"\\nAll steps complete \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
